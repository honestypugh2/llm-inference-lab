{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2fd0e6",
   "metadata": {},
   "source": [
    "# LLM Inference: Introduction to Optimization and Efficiency Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216966c",
   "metadata": {},
   "source": [
    "## Introduction on Summarization\n",
    "\n",
    "Text summarization is a natural language processing task that is the process of creating shorter text from a document or sequence of text that captures the most important information.\n",
    "\n",
    "Summarization can take the following form:\n",
    "\n",
    "- Extractive summarization is the process of extracting the most relevant text from the document and using the relevant text to form a summary.\n",
    "- Abstractive summarization is the process of generating new text that captures the most relevant information from the document. The generated summary may contain text that does not appear in the document.\n",
    "\n",
    "Summarization is an example of a sequence-to-sequence task. It has a family of models that are encoder-decoder models that use both parts of the Transformer architecture. The encoder's attention layers has access to all the words of the input text, while the decoder's attention layers only have access to the words that are positioned before the target word from the input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd196287",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "1. Understand the concept of inference optimization.\n",
    "2. Learn techniques to optimize inference for machine learning models.\n",
    "3. Implement and evaluate these optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b45310",
   "metadata": {},
   "source": [
    "## Set Up Your Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89887ec",
   "metadata": {},
   "source": [
    "### Install Required Libraries \n",
    "\n",
    "Ensure you have the necessary libraries installed. You can install them using pip if they are not already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b9dbd",
   "metadata": {},
   "source": [
    "### Import Libraries \n",
    "\n",
    "Import the necessary libraries for data manipulation, model loading, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PegasusForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    "    PegasusTokenizerFast,\n",
    "    BartTokenizerFast,\n",
    "    BatchEncoding,\n",
    ")\n",
    "\n",
    "from llm_inference_lab.utils.download_models import download_models\n",
    "\n",
    "from llm_inference_lab.utils.models import pegasus_model, distilbart_model\n",
    "\n",
    "from llm_inference_lab.tools.torch.quantization import dynamic_quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f2b11",
   "metadata": {},
   "source": [
    "Unused imports. The imports below will be used in another lab focused on benchmarking and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d53ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from llm_inference_lab.summarization.summarization import TextSummarizer\n",
    "\n",
    "# from llm_inference_lab.utils.benchmark import measure_inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60413a29",
   "metadata": {},
   "source": [
    "### Download Models\n",
    "\n",
    "Download the models for the Lab. Depending on your network connectivity, this may take longer than expected.\n",
    "\n",
    "*Expected download time is approximately 2 minutes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359ab55",
   "metadata": {},
   "source": [
    "More information about the models we will download for this lab can be found here:\n",
    "\n",
    "- [google/pegasus-cnn_dailymail](https://huggingface.co/google/pegasus-cnn_dailymail) - This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned are more extractive.\n",
    "- [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) - This model is a form of a compressed model technique known as distillation. Distillation is the process of transferring knowledge from a larger model, also referred to as the teacher, to a smaller model, also referred as the student. This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned tend to contain snippets of verbatim text from the input document (so may resemble an extractive summary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366ba27",
   "metadata": {},
   "source": [
    "#### Default Hyperparameters for Models used in this Lab\n",
    "\n",
    "**Default hyperparameters for Pegasus**\n",
    "\n",
    "    Model Parameters:​\n",
    "        Tokenization:​\n",
    "            max_length – 512​\n",
    "            padding – True​\n",
    "            truncation – True​\n",
    "    ​\n",
    "    Generation:​\n",
    "        Parameters that control Generation Strategy:​\n",
    "        num_beams – 4 (model default is 8)​\n",
    "\n",
    "    Parameters that control the length of output:​\n",
    "        min_length - 32​\n",
    "        max_length - 128​\n",
    "        early_stopping – True​\n",
    "        max_new_tokens - 128​\n",
    "\n",
    "    Parameters for manipulation of model output logits:​\n",
    "        length_penalty - 0.8​\n",
    "        no_repeat_ngram_size - 0 (default)\n",
    "\n",
    "**Default hyperparameters for DistilBART**\n",
    "\n",
    "    Model Parameters:​\n",
    "        Tokenization:​\n",
    "            max_length – 512​\n",
    "            padding – True​\n",
    "            truncation – True​\n",
    "    ​\n",
    "    Generation:​\n",
    "        Parameters that control Generation Strategy:​\n",
    "            num_beams – 4​\n",
    "\n",
    "    Parameters that control the length of output:​\n",
    "        min_length - 56​\n",
    "        max_length - 142​\n",
    "        early_stopping – True​\n",
    "        max_new_tokens - 128​\n",
    "\n",
    "    Parameters for manipulation of model output logits:​\n",
    "        length_penalty - 2​\n",
    "        no_repeat_ngram_size - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "download_models(all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab736e5",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15987ad",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "\n",
    "For this lab, we will use the text labeled `src_text`.\n",
    "\n",
    "For other examples of text, we will use the [Xsum dataset](https://www.kaggle.com/datasets/mdnaveedmmulla/xsumdataset?resource=download&select=xsum_test.csv), which is a classic dataset for summarization tasks. The use of this dataset is dependent on resources. **When using local resources, you may experience Kernel die issues. Recommended to increase your resources if using Xsum Dataset or if change any model hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/xsum_validation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30634b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"document\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa61d8",
   "metadata": {},
   "source": [
    "We will proceed with `src_text` shown below for the remaining of this lab.\n",
    "\n",
    "The text below original source: [Pegasus Usage Example](https://huggingface.co/docs/transformers/main/model_doc/pegasus#usage-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c8dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = [\n",
    "    \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262e697",
   "metadata": {},
   "source": [
    "*Note: Keep in mind that some models required some text preprocessing before training, fune-tuning, or inference. This lab does not coverage the specifics on text preprocessing techniques but it is advised to consider what kind of text preprocessing is needed to support the model used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443de225",
   "metadata": {},
   "source": [
    "## Load and Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca65b40",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model and Tokenizer \n",
    "\n",
    "For this lab, we will use the Pegasus CNN_Dailymail model, which is a pretrained language model. This will be our base model we will want to optimize for inference.\n",
    "\n",
    "- [google/pegasus-cnn_dailymail](https://huggingface.co/google/pegasus-cnn_dailymail) - This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned are more extractive.\n",
    "- [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) - This model is a form of a compressed model technique known as distillation. Distillation is the process of transferring knowledge from a larger model, also referred to as the teacher, to a smaller model, also referred as the student. This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned tend to contain snippets of verbatim text from the input document (so may resemble an extractive summary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fdbc0",
   "metadata": {},
   "source": [
    "#### Load Base Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = pegasus_model.path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0c87b",
   "metadata": {},
   "source": [
    "#### Load Quantized Model using PyTorch's Dynamic Quantization\n",
    "\n",
    "Dynamic quantization will only be applied to the Pegasus model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = dynamic_quantization(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70ecfc",
   "metadata": {},
   "source": [
    "#### Load Distilled Model: DistilBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dist = distilbart_model.path\n",
    "\n",
    "tokenizer_dist = AutoTokenizer.from_pretrained(model_name_dist)\n",
    "distilled_model = BartForConditionalGeneration.from_pretrained(model_name_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8490229",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb620299",
   "metadata": {},
   "source": [
    "Set device and view model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba93337",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4662a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb2f64",
   "metadata": {},
   "source": [
    "## Optimize Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826b49d",
   "metadata": {},
   "source": [
    "### Enable Model Evaluation Mode \n",
    "\n",
    "Set the model to evaluation mode to disable dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ad188",
   "metadata": {},
   "source": [
    "`model.eval()` - This is not needed for inference with transformers but is needed for torch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8333980",
   "metadata": {},
   "source": [
    "### Optimize Tokenization\n",
    "\n",
    "Tokenize the input text efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a187093",
   "metadata": {},
   "source": [
    "#### Base Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2da19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE - MORE RESOURCE INTENSIVE\n",
    "\n",
    "# inputs = tokenizer(src_text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(src_text, padding=True, truncation=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b5957",
   "metadata": {},
   "source": [
    "*Note: The Quanitized Model created from using PyTorch's Dynamic Quantization uses the same tokenized inputs as the Base Model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171e1f",
   "metadata": {},
   "source": [
    "#### Distilled Model: DistilBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dist = tokenizer_dist(src_text, padding=True, truncation=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfc506",
   "metadata": {},
   "source": [
    "### Optimize Inference with Batch Processing\n",
    "\n",
    "Use batch processing to optimize inference for multiple inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234b113",
   "metadata": {},
   "source": [
    "We will not complete this during the lab but view the following modules for other ways to optimize inference via batch processing, distillation, quantization, and leveraging Ray for distributed processing.\n",
    "\n",
    "- `from llm_inference_lab.summarization.summarization import TextSummarizer`\n",
    "- `from llm_inference_lab.summarization.summarization_ray import TextSummarizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36485bcc",
   "metadata": {},
   "source": [
    "## Generate Summaries\n",
    "\n",
    "Use the optimized inference process to make predictions (generate responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8750167",
   "metadata": {},
   "source": [
    "### Use Base Model: Pegasus model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646098c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "            inputs[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "summaries = tokenizer.batch_decode(outputs,skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568ee8",
   "metadata": {},
   "source": [
    "### Use Quantized Pegasus Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_outputs = quantized_model.generate(\n",
    "            inputs[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "quantized_summaries = tokenizer.batch_decode(quantized_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(quantized_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90c82d",
   "metadata": {},
   "source": [
    "### Use Distilled Model: DistilBART for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_outputs = distilled_model.generate(\n",
    "            inputs_dist[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "distilled_summaries = tokenizer_dist.batch_decode(distilled_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(distilled_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f893c8f",
   "metadata": {},
   "source": [
    "## Evaluate the Optimized Inference\n",
    "\n",
    "Evaluate the performance of the optimized inference process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4ecf9",
   "metadata": {},
   "source": [
    "### Measure Inference Time\n",
    "\n",
    "Measure the time taken for inference before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80442cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model: PegasusForConditionalGeneration\n",
    "    | BartForConditionalGeneration, \n",
    "    tokenizer: PegasusTokenizerFast | BartTokenizerFast,\n",
    "    inputs: BatchEncoding\n",
    "   ):\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = model.generate(\n",
    "        inputs[\"input_ids\"]\n",
    "        )\n",
    "\n",
    "        _ = tokenizer.batch_decode(outputs,skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        \n",
    "        # Record end time and calculate latency\n",
    "        end_time = time.time()\n",
    "        latency = (end_time - start_time)\n",
    "        return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_time = measure_inference_time(model, tokenizer, inputs)\n",
    "optimized_quantized_time = measure_inference_time(quantized_model, tokenizer, inputs)\n",
    "optimized_distilled_time = measure_inference_time(distilled_model, tokenizer_dist, inputs_dist)\n",
    "\n",
    "print(f\"Pegasus Model - Original Inference Time: {original_time:.2f} seconds\")\n",
    "print(f\"Quantized Pegasus Model - Optimized Quantized Inference Time: {optimized_quantized_time:.2f} seconds\")\n",
    "print(f\"DistilBART Model - Optimized Distilled Inference Time: {optimized_distilled_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef969ad",
   "metadata": {},
   "source": [
    "### Evaluate Prediction Accuracy\n",
    "\n",
    "Evaluate the accuracy of the generated summaries using the test dataset (or validation or hold-out set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341a691",
   "metadata": {},
   "source": [
    "**For the purposes of this lab, we will not evaluate prediction accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efd292",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Recall the presentation material and lab content. Spend time answering the questions below or be ready to discuss as a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca7834",
   "metadata": {},
   "source": [
    "**Questions to Ask Yourself:**\n",
    "- When and why would you use a particular technique?\n",
    "- What are ways to determine if inference optimization is necessary?\n",
    "- What do we expect the output to be for the techniques used?\n",
    "- What are ways we measure inference efficiency?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fae19",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "- Load a pre-trained language model.\n",
    "- Optimize the inference process for faster and more efficient predictions.\n",
    "- Evaluate the optimized inference process.\n",
    "\n",
    "This simple inference optimization task demonstrates the basic workflow of using a *smaller* pretrained language model for optimized prediction and evaluation. \n",
    "\n",
    "You can extend this lab by using different models, optimization techniques, and evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_lab_env",
   "language": "python",
   "name": "inference_lab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

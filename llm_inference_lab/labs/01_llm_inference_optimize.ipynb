{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2fd0e6",
   "metadata": {},
   "source": [
    "# LLM Inference: Introduction to Optimization and Efficiency Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216966c",
   "metadata": {},
   "source": [
    "## Introduction on Summarization\n",
    "\n",
    "Text summarization is a natural language processing task that is the process of creating shorter text from a document or sequence of text that captures the most important information.\n",
    "\n",
    "Summarization can take the following form:\n",
    "\n",
    "- Extractive summarization is the process of extracting the most relevant text from the document and using the relevant text to form a summary.\n",
    "- Abstractive summarization is the process of generating new text that captures the most relevant information from the document. The generated summary may contain text that does not appear in the document.\n",
    "\n",
    "Summarization is an example of a sequence-to-sequence task. It has a family of models that are encoder-decoder models that use both parts of the Transformer architecture. The encoder's attention layers has access to all the words of the input text, while the decoder's attention layers only have access to the words that are positioned before the target word from the input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd196287",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "1. Understand the concept of inference optimization.\n",
    "2. Learn techniques to optimize inference for machine learning models.\n",
    "3. Implement and evaluate these optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b45310",
   "metadata": {},
   "source": [
    "## Set Up Your Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89887ec",
   "metadata": {},
   "source": [
    "### Install Required Libraries \n",
    "\n",
    "Ensure you have the necessary libraries installed. You can install them using pip if they are not already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b9dbd",
   "metadata": {},
   "source": [
    "### Import Libraries \n",
    "\n",
    "Import the necessary libraries for data manipulation, model loading, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    PegasusForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    "    PegasusTokenizerFast,\n",
    "    BartTokenizerFast,\n",
    "    BatchEncoding,\n",
    ")\n",
    "\n",
    "from llm_inference_lab.utils.download_models import download_models\n",
    "\n",
    "from llm_inference_lab.utils.models import pegasus_model, distilbart_model\n",
    "\n",
    "from llm_inference_lab.tools.torch.quantization import dynamic_quantization\n",
    "\n",
    "from llm_inference_lab.summarization.summarization import TextSummarizer\n",
    "\n",
    "from llm_inference_lab.utils.benchmark import measure_inference_latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60413a29",
   "metadata": {},
   "source": [
    "### Download Models\n",
    "\n",
    "Download the models for the Lab. Depending on your network connectivity, this may take longer than expected.\n",
    "\n",
    "*Expected download time is approximately 2 minutes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359ab55",
   "metadata": {},
   "source": [
    "More information about the models we will download for this lab can be found here:\n",
    "\n",
    "- [google/pegasus-cnn_dailymail](https://huggingface.co/google/pegasus-cnn_dailymail) - This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned are more extractive.\n",
    "- [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) - This model is a form of a compressed model technique known as distillation. Distillation is the process of transferring knowledge from a larger model, also referred to as the teacher, to a smaller model, also referred as the student. This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned tend to contain snippets of verbatim text from the input document (so may resemble an extractive summary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366ba27",
   "metadata": {},
   "source": [
    "#### Default Hyperparameters for Models in this Lab\n",
    "\n",
    "**Default hyperparameters for Pegasus**\n",
    "\n",
    "    Model Parameters:​\n",
    "        Tokenization:​\n",
    "            max_length – 512​\n",
    "            padding – True​\n",
    "            truncation – True​\n",
    "    ​\n",
    "    Generation:​\n",
    "        Parameters that control Generation Strategy:​\n",
    "        num_beams – 4 (model default is 8)​\n",
    "\n",
    "    Parameters that control the length of output:​\n",
    "        min_length - 32​\n",
    "        max_length - 128​\n",
    "        early_stopping – True​\n",
    "        max_new_tokens - 128​\n",
    "\n",
    "    Parameters for manipulation of model output logits:​\n",
    "        length_penalty - 0.8​\n",
    "        no_repeat_ngram_size - 0 (default)\n",
    "\n",
    "**Default hyperparameters for DistilBART**\n",
    "\n",
    "    Model Parameters:​\n",
    "        Tokenization:​\n",
    "            max_length – 512​\n",
    "            padding – True​\n",
    "            truncation – True​\n",
    "    ​\n",
    "    Generation:​\n",
    "        Parameters that control Generation Strategy:​\n",
    "            num_beams – 4​\n",
    "\n",
    "    Parameters that control the length of output:​\n",
    "        min_length - 56​\n",
    "        max_length - 142​\n",
    "        early_stopping – True​\n",
    "        max_new_tokens - 128​\n",
    "\n",
    "    Parameters for manipulation of model output logits:​\n",
    "        length_penalty - 2​\n",
    "        no_repeat_ngram_size - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cb120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "download_models(all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab736e5",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15987ad",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "\n",
    "For this lab, we will use the [Xsum dataset](https://www.kaggle.com/datasets/mdnaveedmmulla/xsumdataset?resource=download&select=xsum_test.csv), which is a classic dataset for summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/xsum_validation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30634b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"document\"].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = list(df[\"document\"].values[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262e697",
   "metadata": {},
   "source": [
    "*Note: Keep in mind that some models required some text preprocessing before training, fune-tuning, or inference. This lab does not coverage the specifics on text preprocessing techniques but it is advised to consider what kind of text preprocessing is needed to support the model used.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_text = re.sub(\"\\\\s{2,}\", \" \", src_text)\n",
    "# src_text = re.sub(\"\\\\n{2,}\", \"\\n\", src_text).strip()\n",
    "# src_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443de225",
   "metadata": {},
   "source": [
    "## Load and Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca65b40",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model and Tokenizer \n",
    "\n",
    "For this lab, we will use the Pegasus CNN_Dailymail model, which is a pretrained language model. This will be our base model we will want to optimize for inference.\n",
    "\n",
    "- [google/pegasus-cnn_dailymail](https://huggingface.co/google/pegasus-cnn_dailymail) - This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned are more extractive.\n",
    "- [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6) - This model is a form of a compressed model technique known as distillation. Distillation is the process of transferring knowledge from a larger model, also referred to as the teacher, to a smaller model, also referred as the student. This model provides an abstractive summary that is high in extractive coverage/density, which means the summaries returned tend to contain snippets of verbatim text from the input document (so may resemble an extractive summary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fdbc0",
   "metadata": {},
   "source": [
    "#### Load Base Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = pegasus_model.path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0c87b",
   "metadata": {},
   "source": [
    "#### Load Quantized Model using PyTorch's Dynamic Quantization\n",
    "\n",
    "Dynamic quantization will only be applied to the Pegasus model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = dynamic_quantization(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70ecfc",
   "metadata": {},
   "source": [
    "#### Load Distilled Model: DistilBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dist = distilbart_model.path\n",
    "\n",
    "tokenizer_dist = AutoTokenizer.from_pretrained(model_name_dist)\n",
    "distilled_model = BartForConditionalGeneration.from_pretrained(model_name_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa11752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil as ps\n",
    "\n",
    "# ps.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8490229",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba93337",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4662a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb2f64",
   "metadata": {},
   "source": [
    "## Optimize Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826b49d",
   "metadata": {},
   "source": [
    "### Enable Model Evaluation Mode \n",
    "\n",
    "Set the model to evaluation mode to disable dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ad188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8333980",
   "metadata": {},
   "source": [
    "### Optimize Tokenization\n",
    "\n",
    "Tokenize the input text efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a187093",
   "metadata": {},
   "source": [
    "#### Base Model: Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(src_text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b5957",
   "metadata": {},
   "source": [
    "*Note: The Quanitized Model created from using PyTorch's Dynamic Quantization uses the same tokenized inputs as the Base Model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171e1f",
   "metadata": {},
   "source": [
    "#### Distilled Model: DistilBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dist = tokenizer_dist(src_text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bfc506",
   "metadata": {},
   "source": [
    "### Optimize Inference with Batch Processing\n",
    "\n",
    "Use batch processing to optimize inference for multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return predictions.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3fcd3",
   "metadata": {},
   "source": [
    "### Optimize Inference with Dynamic Quantization\n",
    "\n",
    "Use PyTorch's dynamic quantization on the Pegasus model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302c36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "672bdbd1",
   "metadata": {},
   "source": [
    "### Optimize Inference with Distillation\n",
    "\n",
    "Use DistilBart model to demonstrate applying a distilled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57334cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36485bcc",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Use the optimized inference process to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8750167",
   "metadata": {},
   "source": [
    "### Use Base Model: Pegasus model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37632a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optimized inference process to generate text.\n",
    "# input_texts = [\"Once upon a time\", \"In a galaxy far, far away\"]\n",
    "# tokenized_inputs = [tokenize_input(text) for text in input_texts]\n",
    "# batch_inputs = torch.cat(tokenized_inputs, dim=0)\n",
    "# generated_texts = generate_text(batch_inputs)\n",
    "# for i, text in enumerate(generated_texts):\n",
    "#     print(f\"Input: {input_texts[i]}\")\n",
    "#     print(f\"Generated: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646098c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "\n",
    "summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568ee8",
   "metadata": {},
   "source": [
    "### Use Quantized Pegasus Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_outputs = quantized_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "\n",
    "quantized_summaries = tokenizer.batch_decode(quantized_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(quantized_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90c82d",
   "metadata": {},
   "source": [
    "### Use Distilled Model: DistilBART for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57396dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_outputs = distilled_model.generate(\n",
    "            inputs_dist[\"input_ids\"],\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            max_new_tokens=128\n",
    "        )\n",
    "\n",
    "distilled_summaries = tokenizer_dist.batch_decode(distilled_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(distilled_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f893c8f",
   "metadata": {},
   "source": [
    "## Evaluate the Optimized Inference\n",
    "\n",
    "Evaluate the performance of the optimized inference process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4ecf9",
   "metadata": {},
   "source": [
    "### Measure Inference Time\n",
    "\n",
    "Measure the time taken for inference before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1cb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the optimized inference process.\n",
    "# \t1. Measure Inference Time Measure the time taken for inference before and after optimization.\n",
    "#    import time\n",
    "# def measure_inference_time(inputs):\n",
    "#        start_time = time.time()\n",
    "#        predict(inputs)\n",
    "#        end_time = time.time()\n",
    "#        return end_time - start_time\n",
    "# original_time = measure_inference_time(batch_inputs)\n",
    "#    optimized_time = measure_inference_time(batch_inputs)\n",
    "#    print(f\"Original Inference Time: {original_time:.2f} seconds\")\n",
    "#    print(f\"Optimized Inference Time: {optimized_time:.2f} seconds\")\n",
    "# \t2. Evaluate Prediction Accuracy Evaluate the accuracy of the predictions using a sample dataset.\n",
    "#    from sklearn.metrics import accuracy_score\n",
    "# # Assuming y_test contains the true labels for the input_texts\n",
    "#    y_test = [1, 0]  # Example true labels\n",
    "#    accuracy = accuracy_score(y_test, predictions)\n",
    "#    print(f\"Prediction Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea9003",
   "metadata": {},
   "source": [
    "## Measure Inference Time\n",
    "\n",
    "Measure the time taken for inference before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0539e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the time taken for inference before and after optimization.\n",
    "# def measure_inference_time(model, X):\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict_batch(model, X)\n",
    "#     end_time = time.time()\n",
    "#     return end_time - start_time, predictions\n",
    "# original_time, original_predictions = measure_inference_time(loaded_model, X_test)\n",
    "# print(f\"Original Inference Time: {original_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef969ad",
   "metadata": {},
   "source": [
    "## Evaluate the Optimized Inference\n",
    "\n",
    "Evaluate the performance of the optimized inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t1. Evaluate Prediction Accuracy Evaluate the accuracy of the predictions using the test dataset.\n",
    "#    accuracy = accuracy_score(y_test, original_predictions)\n",
    "#    print(f\"Prediction Accuracy: {accuracy:.2f}\")\n",
    "# \t2. Evaluate Inference Time Compare the inference time before and after optimization.\n",
    "#    optimized_time, optimized_predictions = measure_inference_time(loaded_model, X_test)\n",
    "#    print(f\"Optimized Inference Time: {optimized_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c2c42",
   "metadata": {},
   "source": [
    "## Put it All Together: Optimize Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228ebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c6fae19",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "- Load a pre-trained small language model.\n",
    "- Optimize the inference process for faster and more efficient predictions.\n",
    "- Evaluate the optimized inference process.\n",
    "\n",
    "\n",
    "This simple inference optimization task demonstrates the basic workflow of using a small pretrained language model for optimized prediction and evaluation. \n",
    "\n",
    "You can extend this lab by using different models, optimization techniques, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d484d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_lab_env",
   "language": "python",
   "name": "inference_lab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
